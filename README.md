# ğŸ§ SageAxiom

**Fractal Neural Architecture for ARC-2025 Challenges**
*Because pixel-perfect pain builds real generalization.*

---

## âœ¨ VisÃ£o Geral

**SageAxiom** Ã© um modelo de aprendizagem profundo projetado para lidar com tarefas da competiÃ§Ã£o **ARC Prize 2025**. Ele tenta capturar padrÃµes visuais complexos com um sistema de memÃ³ria episÃ³dica, codificaÃ§Ã£o posicional 2D, refinamento de saÃ­da e um sistema punitivo de dor e disciplina... porque aparentemente *"sÃ³ supervisionado"* nÃ£o estava funcionando.

---

## ğŸ§  Arquitetura

* `early_proj`: ProjeÃ§Ã£o convolucional de entrada
* `EnhancedEncoder`: Encoder fractal com blocos residuais e normalizaÃ§Ã£o
* `EpisodicMemory`: AcÃºmulo sequencial de embeddings
* `LongTermMemory`: Banco de vetores fixos que podem ser consultados por similaridade
* `MultiHeadAttentionWrapper`: AtenÃ§Ã£o prÃ³pria sobre o contexto
* `ChoiceHypothesisModule`: GeraÃ§Ã£o e seleÃ§Ã£o ponderada de transformaÃ§Ãµes
* `TaskPainSystem`: Penalidade contextual baseada em softmax, entropia, e emoÃ§Ãµes simuladas
* `OutputRefinement`: Refinamento final com fallback conservador
* `BoundingBoxDiscipline`: Penalidade baseada em Ã¡rea para coibir entusiasmo fora de controle

---

## âš™ï¸ Treinamento

O modelo Ã© treinado com:

* **Loss principal**: diferenÃ§a quadrÃ¡tica pixel a pixel (`MSE`)
* **Auxiliares**: entropia, bounding box penalty, regulaÃ§Ã£o de `gate` e `alpha`
* **Otimizador**: Adam com `learning_rate=3e-4`
* **Ã‰pocas recomendadas**: entre 100â€“300

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
```

---

## ğŸ§ª Modo de InferÃªncia

```python
x_seq, y_seq, test_x, test_input, expected_output = prepare_few_shot_from_task(
    task, shot=5, augment=True
)

for epoch in range(300):
    with tf.GradientTape() as tape:
        logits = model(x_seq, y_seq, training=True)
        loss = loss_fn(y_seq[:, -1], logits) + tf.add_n(model.losses)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

pred = model(test_x, training=False)
```

---

## ğŸ“¦ Requisitos

* TensorFlow 2.11+
* NumPy
* Matplotlib / Seaborn (para visualizaÃ§Ã£o)
* Muita tolerÃ¢ncia Ã  dor

---

## ğŸ“Š MÃ©tricas

* `Pixel Accuracy`: acurÃ¡cia pixel a pixel
* `Perfect Match`: saÃ­das idÃªnticas Ã  ground truth
* Logs internos: `pain`, `exploration`, `alpha`, `gate`, etc.

---

## ğŸ”¥ Filosofia

> â€œSe uma rede convolucional sofre dor o suficiente, ela aprende. Ou pelo menos para de fazer besteira com a softmax.â€
> â€” Felipe Maya Muniz

---

## ğŸš¼ LicenÃ§a

CC BY-ND 4.0
