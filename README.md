ğŸ§  SageAxiom
Fractal Neural Architecture for ARC-2025 Challenges
Because pixel-perfect pain builds real generalization.

âœ¨ VisÃ£o Geral
SageAxiom Ã© um modelo de aprendizagem profundo projetado para lidar com tarefas da competiÃ§Ã£o ARC Prize 2025. Ele tenta capturar padrÃµes visuais complexos com um sistema de memÃ³ria episÃ³dica, codificaÃ§Ã£o posicional 2D, refinamento de saÃ­da e um sistema punitivo de dor e disciplina... porque aparentemente "sÃ³ supervisionado" nÃ£o estava funcionando.

ğŸ§¬ Arquitetura
early_proj: ProjeÃ§Ã£o convolucional de entrada

EnhancedEncoder: Encoder fractal com blocos residuais e normalizaÃ§Ã£o

EpisodicMemory: AcÃºmulo seqÃ¼encial de embeddings (estilo RNN mas com memÃ³ria externa)

LongTermMemory: Banco de vetores fixos que podem ser consultados por similaridade

MultiHeadAttentionWrapper: AtenÃ§Ã£o prÃ³pria sobre o contexto

ChoiceHypothesisModule: GeraÃ§Ã£o e seleÃ§Ã£o ponderada de transformaÃ§Ãµes (estilo ensemble)

TaskPainSystem: Penalidade contextual baseada em softmax, entropia, e emoÃ§Ãµes simuladas (sim, sÃ©rio)

OutputRefinement: Refinamento convolucional final com fallback conservador

BoundingBoxDiscipline: Penalidade baseada em Ã¡rea de bounding box para coibir entusiasmo fora de controle

âš™ï¸ Treinamento
O modelo Ã© treinado com:

Loss principal: diferenÃ§a quadrÃ¡tica pixel a pixel (MSE)

Auxiliares: entropia de distribuiÃ§Ã£o, bounding box penalty, e regulaÃ§Ã£o de gate / alpha do sistema de dor

Otimizador: Adam com learning_rate=3e-4

Ã‰pocas recomendadas: entre 100â€“300 (depende da GPU e da paciÃªncia humana)

python
Copiar
Editar
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
ğŸ§ª Modo de InferÃªncia
python
Copiar
Editar
# Preparar o few-shot
x_seq, y_seq, test_x, test_input, expected_output = prepare_few_shot_from_task(
    task, shot=5, augment=True
)

# Treinar
for epoch in range(300):
    with tf.GradientTape() as tape:
        logits = model(x_seq, y_seq, training=True)
        loss = loss_fn(y_seq[:, -1], logits) + tf.add_n(model.losses)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

# InferÃªncia final
pred = model(test_x, training=False)
ğŸ“¦ Requisitos
TensorFlow 2.11+

NumPy

Matplotlib / Seaborn (para visualizaÃ§Ã£o)

Muita tolerÃ¢ncia Ã  dor

ğŸ“ˆ MÃ©tricas
Pixel Accuracy: acurÃ¡cia pixel a pixel

Perfect Match: quantas saÃ­das foram idÃªnticas Ã  ground truth

Logs de atributos internos como: pain, exploration, alpha, gate, etc.

ğŸ”¥ MotivaÃ§Ã£o FilosÃ³fica
â€œSe uma rede convolucional sofre dor o suficiente, ela aprende. Ou pelo menos para de fazer besteira com a softmax.â€
â€” VocÃª, provavelmente, Ã s 3 da manhÃ£.

ğŸ§¼ LicenÃ§a

CC BY-ND 4.0
